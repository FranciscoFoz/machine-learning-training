{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Word2Vec: treinamento de Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explorando o Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv('/home/franciscofoz/Documents/GitHub/machine-learning-training/Processamento de linguagem natural/Datasets/w2v_treino.csv')\n",
    "df_teste = pd.read_csv('/home/franciscofoz/Documents/GitHub/machine-learning-training/Processamento de linguagem natural/Datasets/w2v_teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Após polêmica, Marine Le Pen diz que abomina n...</td>\n",
       "      <td>A candidata da direita nacionalista à Presidên...</td>\n",
       "      <td>2017-04-28</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macron e Le Pen vão ao 2º turno na França, em ...</td>\n",
       "      <td>O centrista independente Emmanuel Macron e a d...</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/04/187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apesar de larga vitória nas legislativas, Macr...</td>\n",
       "      <td>As eleições legislativas deste domingo (19) na...</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/06/189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Governo antecipa balanço, e Alckmin anuncia qu...</td>\n",
       "      <td>O número de ocorrências de homicídios dolosos ...</td>\n",
       "      <td>2015-07-24</td>\n",
       "      <td>cotidiano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/cotidiano/2015/07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Após queda em maio, a atividade econômica sobe...</td>\n",
       "      <td>A economia cresceu 0,25% no segundo trimestre,...</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2017/08/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>Mural: Há 30 anos, aeroporto não foi bem receb...</td>\n",
       "      <td>Década de 1970. Congonhas já estava superlotad...</td>\n",
       "      <td>2015-01-22</td>\n",
       "      <td>cotidiano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/cotidiano/2015/01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>As notícias sobre Schumacher não são boas, diz...</td>\n",
       "      <td>O ex-presidente da Ferrari Luca di Montezemolo...</td>\n",
       "      <td>2016-04-02</td>\n",
       "      <td>esporte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/esporte/2016/02/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>De olho em R$ 50 bilhões, governo pode concede...</td>\n",
       "      <td>Para fazer caixa, o governo estuda conceder pa...</td>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2017/08/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>Moro deu a Lula o papel de coitadinho</td>\n",
       "      <td>Realizou-se parcialmente o primeiro objetivo d...</td>\n",
       "      <td>2016-06-03</td>\n",
       "      <td>colunas</td>\n",
       "      <td>eliogaspari</td>\n",
       "      <td>http://www1.folha.uol.com.br/colunas/eliogaspa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>Velocidade da aprovação das reformas tem 'exce...</td>\n",
       "      <td>O Banco Central afirmou que a velocidade da ap...</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>mercado</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mercado/2016/10/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      Após polêmica, Marine Le Pen diz que abomina n...   \n",
       "1      Macron e Le Pen vão ao 2º turno na França, em ...   \n",
       "2      Apesar de larga vitória nas legislativas, Macr...   \n",
       "3      Governo antecipa balanço, e Alckmin anuncia qu...   \n",
       "4      Após queda em maio, a atividade econômica sobe...   \n",
       "...                                                  ...   \n",
       "89995  Mural: Há 30 anos, aeroporto não foi bem receb...   \n",
       "89996  As notícias sobre Schumacher não são boas, diz...   \n",
       "89997  De olho em R$ 50 bilhões, governo pode concede...   \n",
       "89998              Moro deu a Lula o papel de coitadinho   \n",
       "89999  Velocidade da aprovação das reformas tem 'exce...   \n",
       "\n",
       "                                                    text        date  \\\n",
       "0      A candidata da direita nacionalista à Presidên...  2017-04-28   \n",
       "1      O centrista independente Emmanuel Macron e a d...  2017-04-23   \n",
       "2      As eleições legislativas deste domingo (19) na...  2017-06-19   \n",
       "3      O número de ocorrências de homicídios dolosos ...  2015-07-24   \n",
       "4      A economia cresceu 0,25% no segundo trimestre,...  2017-08-17   \n",
       "...                                                  ...         ...   \n",
       "89995  Década de 1970. Congonhas já estava superlotad...  2015-01-22   \n",
       "89996  O ex-presidente da Ferrari Luca di Montezemolo...  2016-04-02   \n",
       "89997  Para fazer caixa, o governo estuda conceder pa...  2017-08-29   \n",
       "89998  Realizou-se parcialmente o primeiro objetivo d...  2016-06-03   \n",
       "89999  O Banco Central afirmou que a velocidade da ap...  2016-10-25   \n",
       "\n",
       "        category  subcategory  \\\n",
       "0          mundo          NaN   \n",
       "1          mundo          NaN   \n",
       "2          mundo          NaN   \n",
       "3      cotidiano          NaN   \n",
       "4        mercado          NaN   \n",
       "...          ...          ...   \n",
       "89995  cotidiano          NaN   \n",
       "89996    esporte          NaN   \n",
       "89997    mercado          NaN   \n",
       "89998    colunas  eliogaspari   \n",
       "89999    mercado          NaN   \n",
       "\n",
       "                                                    link  \n",
       "0      http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "1      http://www1.folha.uol.com.br/mundo/2017/04/187...  \n",
       "2      http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
       "3      http://www1.folha.uol.com.br/cotidiano/2015/07...  \n",
       "4      http://www1.folha.uol.com.br/mercado/2017/08/1...  \n",
       "...                                                  ...  \n",
       "89995  http://www1.folha.uol.com.br/cotidiano/2015/01...  \n",
       "89996  http://www1.folha.uol.com.br/esporte/2016/02/1...  \n",
       "89997  http://www1.folha.uol.com.br/mercado/2017/08/1...  \n",
       "89998  http://www1.folha.uol.com.br/colunas/eliogaspa...  \n",
       "89999  http://www1.folha.uol.com.br/mercado/2016/10/1...  \n",
       "\n",
       "[90000 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'Rio de Janeiro é uma cidade maravilhosa'\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rio"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Rio de Janeiro,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pré processamento com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_para_tratamento = (titulos.lower() for titulos in df_treino.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_textos(doc):\n",
    "    \n",
    "    tokens_validos = list()\n",
    "    \n",
    "    for token in doc:\n",
    "        eh_valido = not token.is_stop and token.is_alpha\n",
    "        if eh_valido:\n",
    "            tokens_validos.append(token.text)\n",
    "            \n",
    "    if len(tokens_validos) > 2:\n",
    "        return ' '.join(tokens_validos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rio Janeiro cidade maravilhosa'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trata_textos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8640616059303283\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "textos_tratados = [trata_textos(doc) for doc in nlp.pipe(texto_para_tratamento,batch_size=1000,n_process= -1)]\n",
    "\n",
    "tf = time() - t0\n",
    "\n",
    "print(tf/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos_tratados = pd.DataFrame({'titulos':textos_tratados})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queda maio atividade econômica sobe junho bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>mural há anos aeroporto recebido moradores gua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>notícias schumacher boas ferrari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>olho bilhões governo conceder áreas petróleo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>moro deu lula papel coitadinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>velocidade aprovação reformas excedido expecta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 titulos\n",
       "0      polêmica marine le pen abomina negacionistas h...\n",
       "1      macron le pen turno frança revés siglas tradic...\n",
       "2      apesar larga vitória legislativas macron terá ...\n",
       "3      governo antecipa balanço alckmin anuncia queda...\n",
       "4           queda maio atividade econômica sobe junho bc\n",
       "...                                                  ...\n",
       "89995  mural há anos aeroporto recebido moradores gua...\n",
       "89996                   notícias schumacher boas ferrari\n",
       "89997       olho bilhões governo conceder áreas petróleo\n",
       "89998                     moro deu lula papel coitadinho\n",
       "89999  velocidade aprovação reformas excedido expecta...\n",
       "\n",
       "[90000 rows x 1 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos_tratados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hiperparâmetros do Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2v_modelo = Word2Vec(sg=0,\n",
    "                      window=2,\n",
    "                      vector_size=300,\n",
    "                      min_count=5,\n",
    "                      alpha=0.03,\n",
    "                      min_alpha=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "84466\n"
     ]
    }
   ],
   "source": [
    "print(len(titulos_tratados))\n",
    "\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "\n",
    "print(len(titulos_tratados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lista_tokens = [titulo.split(' ') for titulo in titulos_tratados.titulos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 19:34:49,147 : - collecting all words and their counts\n",
      "2024-04-30 19:34:49,149 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-04-30 19:34:49,160 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-04-30 19:34:49,176 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-04-30 19:34:49,189 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-04-30 19:34:49,200 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-04-30 19:34:49,213 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-04-30 19:34:49,226 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-04-30 19:34:49,240 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-04-30 19:34:49,252 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-04-30 19:34:49,263 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-04-30 19:34:49,275 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-04-30 19:34:49,290 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-04-30 19:34:49,302 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-04-30 19:34:49,313 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-04-30 19:34:49,325 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-04-30 19:34:49,340 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-04-30 19:34:49,353 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-04-30 19:34:49,362 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-04-30 19:34:49,364 : - Creating a fresh vocabulary\n",
      "2024-04-30 19:34:49,430 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-04-30T19:34:49.430206', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-04-30 19:34:49,431 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-04-30T19:34:49.431605', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-04-30 19:34:49,559 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-04-30 19:34:49,560 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-04-30 19:34:49,561 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-04-30T19:34:49.561218', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2024-04-30 19:34:49,742 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-04-30 19:34:49,743 : - resetting layer weights\n",
      "2024-04-30 19:34:49,765 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-04-30T19:34:49.765759', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : - %(message)s',level = logging.INFO)\n",
    "\n",
    "w2v_modelo.build_vocab(lista_lista_tokens,progress_per=5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Treinamento do Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 19:34:49,776 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-04-30T19:34:49.776170', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "2024-04-30 19:34:50,530 : - EPOCH 0: training on 540242 raw words (486188 effective words) took 0.7s, 658429 effective words/s\n",
      "2024-04-30 19:34:51,264 : - EPOCH 1: training on 540242 raw words (486151 effective words) took 0.7s, 668481 effective words/s\n",
      "2024-04-30 19:34:52,235 : - EPOCH 2: training on 540242 raw words (486229 effective words) took 1.0s, 504016 effective words/s\n",
      "2024-04-30 19:34:53,249 : - EPOCH 3 - PROGRESS: at 100.00% examples, 484598 words/s, in_qsize 0, out_qsize 1\n",
      "2024-04-30 19:34:53,250 : - EPOCH 3: training on 540242 raw words (486034 effective words) took 1.0s, 483777 effective words/s\n",
      "2024-04-30 19:34:54,204 : - EPOCH 4: training on 540242 raw words (486181 effective words) took 0.9s, 517038 effective words/s\n",
      "2024-04-30 19:34:55,128 : - EPOCH 5: training on 540242 raw words (486125 effective words) took 0.9s, 536909 effective words/s\n",
      "2024-04-30 19:34:55,839 : - EPOCH 6: training on 540242 raw words (486174 effective words) took 0.7s, 691168 effective words/s\n",
      "2024-04-30 19:34:56,553 : - EPOCH 7: training on 540242 raw words (486118 effective words) took 0.7s, 687887 effective words/s\n",
      "2024-04-30 19:34:57,285 : - EPOCH 8: training on 540242 raw words (486071 effective words) took 0.7s, 671522 effective words/s\n",
      "2024-04-30 19:34:58,008 : - EPOCH 9: training on 540242 raw words (486202 effective words) took 0.7s, 681724 effective words/s\n",
      "2024-04-30 19:34:58,705 : - EPOCH 10: training on 540242 raw words (486101 effective words) took 0.7s, 705009 effective words/s\n",
      "2024-04-30 19:34:59,393 : - EPOCH 11: training on 540242 raw words (486169 effective words) took 0.7s, 714006 effective words/s\n",
      "2024-04-30 19:35:00,094 : - EPOCH 12: training on 540242 raw words (486103 effective words) took 0.7s, 703567 effective words/s\n",
      "2024-04-30 19:35:00,793 : - EPOCH 13: training on 540242 raw words (486086 effective words) took 0.7s, 705493 effective words/s\n",
      "2024-04-30 19:35:01,496 : - EPOCH 14: training on 540242 raw words (486073 effective words) took 0.7s, 699065 effective words/s\n",
      "2024-04-30 19:35:02,186 : - EPOCH 15: training on 540242 raw words (486169 effective words) took 0.7s, 715261 effective words/s\n",
      "2024-04-30 19:35:02,873 : - EPOCH 16: training on 540242 raw words (486080 effective words) took 0.7s, 715421 effective words/s\n",
      "2024-04-30 19:35:03,546 : - EPOCH 17: training on 540242 raw words (486167 effective words) took 0.7s, 729761 effective words/s\n",
      "2024-04-30 19:35:04,230 : - EPOCH 18: training on 540242 raw words (486090 effective words) took 0.7s, 717661 effective words/s\n",
      "2024-04-30 19:35:04,953 : - EPOCH 19: training on 540242 raw words (486145 effective words) took 0.7s, 679276 effective words/s\n",
      "2024-04-30 19:35:05,619 : - EPOCH 20: training on 540242 raw words (486256 effective words) took 0.7s, 737931 effective words/s\n",
      "2024-04-30 19:35:06,299 : - EPOCH 21: training on 540242 raw words (486204 effective words) took 0.7s, 722626 effective words/s\n",
      "2024-04-30 19:35:06,982 : - EPOCH 22: training on 540242 raw words (486266 effective words) took 0.7s, 720034 effective words/s\n",
      "2024-04-30 19:35:07,758 : - EPOCH 23: training on 540242 raw words (486151 effective words) took 0.8s, 634229 effective words/s\n",
      "2024-04-30 19:35:08,520 : - EPOCH 24: training on 540242 raw words (486132 effective words) took 0.8s, 644072 effective words/s\n",
      "2024-04-30 19:35:09,295 : - EPOCH 25: training on 540242 raw words (486103 effective words) took 0.8s, 635992 effective words/s\n",
      "2024-04-30 19:35:09,951 : - EPOCH 26: training on 540242 raw words (486218 effective words) took 0.6s, 748607 effective words/s\n",
      "2024-04-30 19:35:10,634 : - EPOCH 27: training on 540242 raw words (486073 effective words) took 0.7s, 719771 effective words/s\n",
      "2024-04-30 19:35:11,302 : - EPOCH 28: training on 540242 raw words (486270 effective words) took 0.7s, 735496 effective words/s\n",
      "2024-04-30 19:35:11,996 : - EPOCH 29: training on 540242 raw words (486132 effective words) took 0.7s, 716114 effective words/s\n",
      "2024-04-30 19:35:11,997 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584461 effective words) took 22.2s, 656386 effective words/s', 'datetime': '2024-04-30T19:35:11.997523', 'gensim': '4.3.2', 'python': '3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]', 'platform': 'Linux-6.5.0-28-generic-x86_64-with-glibc2.35', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584461, 16207260)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.train(lista_lista_tokens,\n",
    "                 total_examples=w2v_modelo.corpus_count,\n",
    "                 epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('playboy', 0.4001206159591675),\n",
       " ('processada', 0.3645883798599243),\n",
       " ('hugh', 0.3630234897136688),\n",
       " ('capa', 0.3609415888786316),\n",
       " ('aliança', 0.3601062297821045),\n",
       " ('abbey', 0.3542258143424988),\n",
       " ('anônimos', 0.3495810329914093),\n",
       " ('got', 0.3494630455970764),\n",
       " ('allen', 0.345334529876709),\n",
       " ('depardieu', 0.3414243459701538)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.wv.most_similar('revista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biografia', 0.4514586627483368),\n",
       " ('romance', 0.4380078613758087),\n",
       " ('antologia', 0.43577247858047485),\n",
       " ('virginia', 0.43511706590652466),\n",
       " ('livros', 0.4023575782775879),\n",
       " ('houellebecq', 0.4016284942626953),\n",
       " ('memórias', 0.3980974853038788),\n",
       " ('autobiografia', 0.39702656865119934),\n",
       " ('coletânea', 0.3951440453529358),\n",
       " ('cd', 0.3836880028247833)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.wv.most_similar('livro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cultivo', 0.5860814452171326),\n",
       " ('gastronomia', 0.45166119933128357),\n",
       " ('publicados', 0.4361380338668823),\n",
       " ('homicídios', 0.434289813041687),\n",
       " ('culinária', 0.4296986758708954),\n",
       " ('termômetro', 0.4270356595516205),\n",
       " ('computadores', 0.41144418716430664),\n",
       " ('assinantes', 0.40799835324287415),\n",
       " ('firmam', 0.4076526463031769),\n",
       " ('puxada', 0.4010506868362427)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.wv.most_similar('coca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('orson', 0.52555251121521),\n",
       " ('paixão', 0.4877203404903412),\n",
       " ('viúva', 0.4702746868133545),\n",
       " ('genial', 0.46040648221969604),\n",
       " ('brincalhão', 0.45456767082214355),\n",
       " ('achei', 0.44957220554351807),\n",
       " ('neta', 0.4491339325904846),\n",
       " ('banal', 0.44517019391059875),\n",
       " ('dela', 0.44451817870140076),\n",
       " ('cocaína', 0.44066253304481506)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.wv.most_similar('gato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm', disable=['parser','ner','tagger','textcat'])\n",
    "\n",
    "def tokenizador(texto):\n",
    "    \n",
    "    doc = nlp(texto)\n",
    "    tokens_validos = list()\n",
    "    for token in doc:\n",
    "        eh_valido = not token.is_stop and token.is_alpha\n",
    "        if eh_valido:\n",
    "            tokens_validos.append(token.text.lower())\n",
    "    \n",
    "    return tokens_validos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'Rio de Janeiro 12213323 ***** @#$ é uma cidade maravilhosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rio', 'janeiro', 'cidade', 'maravilhosa']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizador(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def combinacao_de_vetores_por_soma(palavras,modelo):\n",
    "    \n",
    "    vetor_resultante = np.zeros(300)\n",
    "    \n",
    "    for pn in palavras:\n",
    "        \n",
    "        try:\n",
    "            vetor_resultante += modelo.get_vector(pn)\n",
    "        except KeyError:\n",
    "            pass\n",
    "                \n",
    "    return vetor_resultante\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_vetores(textos):\n",
    "    x = len(textos)\n",
    "    y = 300\n",
    "    \n",
    "    matriz = np.zeros((x,y))\n",
    "    \n",
    "    for i in range(x):\n",
    "        palavras_numeros = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_de_vetores_por_soma(palavras_numeros)\n",
    "        \n",
    "    return matriz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
